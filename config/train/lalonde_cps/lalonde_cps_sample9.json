{
    "filepaths": {
        "dag_g_formula": "config/dag/lalonde_cps_g_formula_dag.json",
        "dag_ipw": "config/dag/lalonde_cps_ipw_dag.json",
        "dag_aipw": "config/dag/lalonde_cps_aipw_dag.json",
        "config": "config/train/lalonde_cps/lalonde_cps_sample9.json",
        "data_file": "data/lalonde/ldw_cps/ldw_cps.csv",
        "data_train_file": "data/lalonde/ldw_cps/sample9/train_data_9.csv",
        "data_val_file": "data/lalonde/ldw_cps/sample9/val_data_9.csv",
        "data_test_file": "data/lalonde/ldw_cps/sample9/test_data_9.csv",
        "pseudo_ate_file": "src/train/lalonde/lalonde_cps/aipw_grf_pseudo_ate.csv",
        "predictions_g-formula": "experiments/results/lalonde_cps/predictions_g_formula_sample9.csv",
        "predictions_ipw": "experiments/results/lalonde_cps/predictions_ipw_sample9.csv",
        "predictions_aipw": "experiments/results/lalonde_cps/predictions_aipw_sample9.csv"
    },
    "random_seed": 42,
  "aipw": {
    "model": {
      "network_width": 512,
      "embedding_dim": 256,
      "feedforward_dim": 1024,
      "num_heads": 1,
      "num_layers": 2,
      "dropout_rate": 0.001,
      "activation": "relu",
      "use_layernorm": false,
      "input_layer_depth": 1,
      "encoder_weight": 0.01
    },
    "training": {
      "n_epochs": 100,
      "batch_size": 256,
      "learning_rate": 0.001,
      "l2_penalty": 1e-6,
      "dag_attention_mask": false
    }
  },
 "g-formula": {
    "model": {
      "network_width": 512,
      "embedding_dim": 256,
      "feedforward_dim": 1024,
      "num_heads": 1,
      "num_layers": 1,
      "dropout_rate": 0.001,
      "activation": "relu",
      "use_layernorm": false,
      "input_layer_depth": 4,
      "encoder_weight": 0
    },
    "training": {
      "n_epochs": 300,
      "batch_size": 256,
      "learning_rate": 0.001,
      "l2_penalty": 1e-6,
      "dag_attention_mask": true
    }
  },
 "ipw": {
    "model": {
      "network_width": 512,
      "embedding_dim": 256,
      "feedforward_dim": 1024,
      "num_heads": 1,
      "num_layers": 2,
      "dropout_rate": 0.001,
      "activation": "relu",
      "use_layernorm": false,
      "input_layer_depth": 1,
      "encoder_weight": 0.01
    },
    "training": {
      "n_epochs": 1,
      "batch_size": 64,
      "learning_rate": 0.001,
      "l2_penalty": 1e-6,
      "dag_attention_mask": true
    }
  }
}