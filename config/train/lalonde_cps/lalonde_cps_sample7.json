{
    "filepaths": {
        "dag_g_formula": "config/dag/lalonde_cps_g_formula_dag.json",
        "dag_ipw": "config/dag/lalonde_cps_ipw_dag.json",
        "dag_aipw": "config/dag/lalonde_cps_aipw_dag.json",
        "config": "config/train/lalonde_cps/lalonde_cps_sample7.json",
        "data_file": "data/lalonde/ldw_cps/ldw_cps.csv",
        "data_train_file": "data/lalonde/ldw_cps/sample7/train_data_7.csv",
        "data_val_file": "data/lalonde/ldw_cps/sample7/val_data_7.csv",
        "data_test_file": "data/lalonde/ldw_cps/sample7/test_data_7.csv",
        "pseudo_ate_file": "src/train/lalonde/lalonde_cps/aipw_grf_pseudo_ate.csv",
        "predictions_g-formula": "experiments/results/lalonde_cps/predictions_g_formula_sample7.csv",
        "predictions_ipw": "experiments/results/lalonde_cps/predictions_ipw_sample7.csv",
        "predictions_aipw": "experiments/results/lalonde_cps/predictions_aipw_sample7.csv"
    },
    "random_seed": 42,
  "aipw": {
    "model": {
      "network_width": 512,
      "embedding_dim": 256,
      "feedforward_dim": 1024,
      "num_heads": 1,
      "num_layers": 1,
      "dropout_rate": 0.001,
      "activation": "relu",
      "use_layernorm": false,
      "input_layer_depth": 1,
      "encoder_weight": 0.01
    },
    "training": {
      "n_epochs": 100,
      "batch_size": 256,
      "learning_rate": 0.001,
      "l2_penalty": 1e-6,
      "dag_attention_mask": true
    }
  },
  "g-formula": {
    "model": {
      "network_width": 512,
      "embedding_dim": 256,
      "feedforward_dim": 1024,
      "num_heads": 1,
      "num_layers": 1,
      "dropout_rate": 0.001,
      "activation": "relu",
      "use_layernorm": false,
      "input_layer_depth": 4,
      "encoder_weight": 0
    },
    "training": {
      "n_epochs": 51,
      "batch_size": 256,
      "learning_rate": 0.001,
      "l2_penalty": 1e-6,
      "dag_attention_mask": false
    }
  },
  "ipw": {
    "model": {
      "network_width": 512,
      "embedding_dim": 256,
      "feedforward_dim": 1024,
      "num_heads": 1,
      "num_layers": 2,
      "dropout_rate": 0.001,
      "activation": "relu",
      "use_layernorm": false,
      "input_layer_depth": 4,
      "encoder_weight": 0.01
    },
    "training": {
      "n_epochs": 5,
      "batch_size": 64,
      "learning_rate": 0.001,
      "l2_penalty": 1e-6,
      "dag_attention_mask": true
    }
  }
}